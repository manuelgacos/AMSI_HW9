---
title: "Hw9"
author: "Manuel Alejandro Garcia Acosta"
date: "11/9/2019"
output: pdf_document
---

```{r}
library(MASS) # Contains function boxcox
setwd('/home/noble_mannu/Documents/PhD/First/STAT_2131_Applied_Statistical_Methods_I/HW9')
```

# Homework 9

# Exercise 1

## Part (a)

Here we regress gamble onto the predictors sex, status, income and verbal and show the summary of the model.

```{r}
Data.a <- read.table('Gambling.txt', header = TRUE)
linMod.a <- lm(gamble ~ ., data = Data.a)
summary(linMod.a)
```

Next we plot the residual plots.

### Residuals plots

#### Plot the residuals against sex

```{r}
plot(Data.a$sex, resid(linMod.a), xlab = "Sex", ylab = "Residuals", 
     main = 'Residual plot against sex', col = 'blue')
abline(a=0, b=0)
```

#### Plot the residuals against status

```{r}
plot(Data.a$status, resid(linMod.a), xlab = "Status", ylab = "Residuals", 
     main = 'Residual plot against status', col = 'blue')
abline(a=0, b=0)
```

#### Plot the residuals against income

```{r}
plot(Data.a$income, resid(linMod.a), xlab = "Income", ylab = "Residuals", 
     main = 'Residual plot against income', col = 'blue')
abline(a=0, b=0)
```

#### Plot the residuals against verbal

```{r}
plot(Data.a$verbal, resid(linMod.a), xlab = "Verbal", ylab = "Residuals", 
     main = 'Residual plot against verbal', col = 'blue')
abline(a=0, b=0)
```

#### Plot the residuals against fitted values

```{r}
plot(linMod.a$fitted.values, resid(linMod.a), xlab = "Fitted Values", 
     ylab = "Residuals", main = 'Residual plot against fitted values', 
     col = 'blue')
abline(a=0, b=0)
```

By looking at the residual plots with 'income' and 'fitted values' I think that there might be evidence that the constant variance assumption is being violated. With 'income' there seems to be more spread while this variable gets bigger. In addition, I think that the distribution of the points in the plot vs fitted values leads to think that the variance is not constant.

#### Q-Q Plot

```{r}
qqnorm(residuals(linMod.a)) 
qqline(residuals(linMod.a), col="red")
```

In the Q-Q Plot we can see that both tails deviate. I would be a little bit worried that the errors aren't normally distributed because of what we see.

## Part (b)

In this part we use Box Cox to suggest a transformation for the response variable. The transformed variable, $\tilde{Y}$ will satisfy the usual mean and variance assumptions.

NOTE: In order to use the boxcox() function we add a small value $\delta = 10^{-8}$ to the column $gamble$ of our dataset to get such function to work.

```{r}
Data.b <- data.frame(Data.a) # We make a copy of the original data
Data.b$gamble <- Data.b$gamble + 10^(-8) # We add 10^{-8} to column gamble
linMod.b <- lm(gamble ~ ., data = Data.b) # We run the regression model
summary(linMod.b) # To display the summary of the model
```

Next we'll run the boxcox() function to determine which value for $\lambda$ is reasonable for the transformation. The transformed data will have the form:

$$\tilde{Y} = \frac{Y^{\lambda}-1}{\lambda}$$

```{r}
boxcox.b <- boxcox(linMod.b, plotit = T)  # Run boxcox()
argmax.b <- which.max(boxcox.b$y) # We get the index for argmax(log-likelihood)
lambda.b <- boxcox.b$x[argmax.b] # We get the value for the argmax
```

We get that the best value for $\lambda$ is:

```{r}
lambda.b
```

Here we obtain $\tilde{Y}$ and we plot it against the residuals of the first model we ran in part(b). In addition, we run another regression model with $\tilde{Y}$ as response.

NOTE: I discussed this problem with the professor and he told me that for simplicity's sake (i.e. being able to explain the transformation if we were required) I should go for $\lambda = 0.25$ instead of the value I got above. So I will use such value rather than $\lambda = 0.2222222$. The reason is that the transformation with the first value will be more easily understood.

$$Y^{0.25} = Y^{\frac{1}{4}} = \sqrt{\sqrt{Y}}$$

```{r}
lambda.b.1 <- 0.25 # We'll use the value suggested by the instructor
```

```{r}
Y.tilde.b <- (Data.b$gamble^lambda.b.1 - 1)/lambda.b.1 # We transform Y
# Run regression with Y_tilde as response
linMod.b.2 <- lm(Y.tilde.b ~ sex+status+income+verbal, data = Data.b)
summary(linMod.b.2)
```

### Residual plots under the new model

Here we plot $\tilde{Y}$ vs the estimated residuals.

```{r}
plot(Y.tilde.b, linMod.b.2$residuals, xlab="Y_tilde", 
     ylab="Box-Cox residuals")
```

Here we plot the fitted values vs the estimated residuals.

```{r}
plot(linMod.b.2$fitted.values, linMod.b.2$residuals, xlab="Fitted values", 
     ylab="Box-Cox residuals")
```

### Q-Q Plot under the new model

```{r}
qqnorm(residuals(linMod.b.2)) 
qqline(residuals(linMod.b.2), col="red")
```

The new model appears to satisfy the constant variance assumption and the residuals also appear to follow a normal distribution.

## Part (c)

Next we'll compute the hat matrix $H$ for the models we used in part(b). 

NOTE: Since we used the same predictors, the hat matrix will be the same for the two models we ran in part(b).

```{r}
intercept <- rep(1, length(Data.b$gamble)) # We create the 1 vector
predictors <- data.matrix(Data.b[,1:4]) # We obtain the columns for the predictors
X <- cbind(intercept,predictors) # We create the X matrix
H <- X %*% solve(t(X) %*% X) %*% t(X) # We create the hat matrix H
```

Remember that the leverage scores are the elements $h_{ii}, i\in \{1,...,n\}$ in the diagonal of the hat matrix $H$. Next we obtain the leverage scores and plot a histogram of them.

```{r}
lev_scores <- diag(H) # We obtain the leverage scores
hist(lev_scores)
```

### Part (c.i)

We are concerned about having large leverage scores because they determine influential point in model fitting. In what follows we check if we have large leverage points in this data.

```{r}
# Here we compute the mean leverage h_bar
p <- 5
n <- length(Data.b$gamble)
mean_lev <- p/n
outliers <- which(lev_scores > 2 * mean_lev)
outliers
```

It seems that the observations 31,33,35 and 42 are influential points in regression our model.

### Part (c.ii)

Here we will reestimate the model from part(b) after removing the points with large leverage scores. I ran again the function boxcox() for the model after deleting the large leverage points to get a new $\lambda$ for the transformation of $Y$.

```{r}
# Here we create a dataset dropping the large leverage points
Data.c <- data.frame(Data.b[-c(31,33,35,42),])
# We regress gamble onto the other variables
linMod.c.1 <- lm(gamble ~ . , data = Data.c)
boxcox.c <- boxcox(linMod.c.1, plotit = T) # Run boxcox() again
```

```{r}
argmax.c <- which.max(boxcox.c$y) # We get the index for argmax(log-likelihood)
lambda.c <- boxcox.c$x[argmax.c] # We get the value for the argmax
lambda.c
```

Here we obtained a different $\lambda$. We proceed to obtain $\tilde{Y}$ once again and run the model regressing it onto the other four predictors.

```{r}
# We obtain Y_tilde once again
Y.tilde.c <- (Data.c$gamble^lambda.c - 1)/lambda.c
linMod.c.2 <- lm(Y.tilde.c ~ sex+status+income+verbal, data = Data.c)
summary(linMod.c.2)
```

```{r}
summary(linMod.b.2)
```

Compared to the model in part(b).

## Part (d)

```{r}
dis.b <- cooks.distance(linMod.b.2) # Retrieve the cook's distance for all points
cut.b <- qf(.50, df1 = 5, df2 = 47-5)
inf.b <- which(dis.b > cut.b)
```

```{r}
dis.c <- cooks.distance(linMod.c.2) # Retrieve the cook's distance for all points
cut.c <- qf(.50, df1 = 5, df2 = 43-5)
inf.c <- which(dis.b > cut.b)
```

None of them seem to be influential points.